{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7q1DhWpV-Dw"
   },
   "source": [
    "# 1st order Training (optimization) methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1670660656528,
     "user_tz": -60
    },
    "id": "Si_DpGyVWZ5I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwSbBfWhP-Bu"
   },
   "source": [
    "Let us consider the following function\n",
    "\n",
    "$$\n",
    "f(x) = e^{-\\frac{x}{10}}\\sin(x) + \\frac{1}{10} \\cos(\\pi x)\n",
    "$$\n",
    "\n",
    "defined over the interval $[0, 10]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1670660656529,
     "user_tz": -60
    },
    "id": "7ICcIcLxyVr7"
   },
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(x) * np.exp(-0.1 * x) + 0.1 * np.cos(np.pi * x)\n",
    "a, b = 0, 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EfJ9uIDoyYNo"
   },
   "source": [
    "Define a function `get_training_data` that returns a collection of `N` training samples, adding a noise sampled from a normal distribution with zero mean and standard deviation `noise`. The output should be a pair of `numpy` matrices `x, y`, of dimension `N` times 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670660656529,
     "user_tz": -60
    },
    "id": "HMn5CmgkzCN6"
   },
   "outputs": [],
   "source": [
    "def get_training_data(N, noise):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    x = np.linspace(a, b, N)[:, None]\n",
    "    y = f(x) + noise * np.random.randn(N, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZSKLNsVzCqg"
   },
   "source": [
    "Plot now the function $f(x)$ in the considered interval, together with 100 training samples with noise magnitude equal to 0.05.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1670660656882,
     "user_tz": -60
    },
    "id": "RPAcbompXSlD",
    "outputId": "237286f8-09fc-47c2-aba9-b24e1c5781d0"
   },
   "outputs": [],
   "source": [
    "x_fine = np.linspace(a, b, 1000)[:, None]\n",
    "plt.plot(x_fine, f(x_fine))\n",
    "\n",
    "xx, yy = get_training_data(100, 0.05)\n",
    "plt.plot(xx, yy, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pycg5Qe0zPby"
   },
   "source": [
    "Write a function `initialize_params` that, given the input `layers_size = [n1, n2, ..., nL]`, generates the parameters associated with an ANN, having as many layers as the number of elements of `layers_size`, with as many neurons as `n1`, `n2`, etc.\n",
    "\n",
    "Inizialize weights sampling from a standard Gaussian distribution and biases with zero values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670660656882,
     "user_tz": -60
    },
    "id": "yw4yIqagbwiV"
   },
   "outputs": [],
   "source": [
    "def initialize_params(layers_size):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    params = list()\n",
    "    for i in range(len(layers_size) - 1):\n",
    "        W = np.random.randn(layers_size[i + 1], layers_size[i])\n",
    "        b = np.zeros((layers_size[i + 1], 1))\n",
    "        params.append(W)\n",
    "        params.append(b)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTUWVV-G0Wc3"
   },
   "source": [
    "Write a function `ANN` that implements an ANN, given the parameters `params`. Use $\\tanh$ as activation function and do not apply the activation function to the last layer.\n",
    "\n",
    "By convention, both the input and the output have:\n",
    "\n",
    "- 1 sample per row\n",
    "- 1 feature per column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670660656882,
     "user_tz": -60
    },
    "id": "Oap_WrrN0O4e"
   },
   "outputs": [],
   "source": [
    "def ANN(x, params):\n",
    "    # layer = x.T\n",
    "    layer = (2 * x.T - (a + b)) / (b - a)\n",
    "    num_layers = int(len(params) / 2 + 1)\n",
    "    weights = params[0::2]\n",
    "    biases = params[1::2]\n",
    "    for i in range(num_layers - 1):\n",
    "        layer = jnp.dot(weights[i], layer) - biases[i]\n",
    "        if i < num_layers - 2:\n",
    "            layer = jnp.tanh(layer)\n",
    "    return layer.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N49WdZR10sGu"
   },
   "source": [
    "Implement a function `loss`, that, given the input `x`, the target output `y` (i.e. the \"labels\") and the parameters `params`, returns the quadratic loss, defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\mathrm{ANN}(x_i, \\boldsymbol{\\theta}))^2\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples in `x`, `y` and $\\boldsymbol{\\theta}$ are the ANN parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670660656883,
     "user_tz": -60
    },
    "id": "oXKxbBBa0PQ7"
   },
   "outputs": [],
   "source": [
    "def loss(x, y, params):\n",
    "    error = ANN(x, params) - y\n",
    "    return jnp.mean(error * error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAfzem6L1N3p"
   },
   "source": [
    "Test your code, by generating the parameters associated with an ANN with two hidden layers with 5 neurons each and by computing the associated loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1670660657126,
     "user_tz": -60
    },
    "id": "7N5LWgbg0TjD",
    "outputId": "ca1ac3fb-4cbf-484e-e3ae-5cadeba66410"
   },
   "outputs": [],
   "source": [
    "params = initialize_params([1, 5, 5, 1])\n",
    "loss(xx, yy, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZB2beeY1aJx"
   },
   "source": [
    "The following cell provides an helper class that allows pnline plots during the training loop. Just run the cell, we will use it later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670660657127,
     "user_tz": -60
    },
    "id": "cGNzV6j8E02t"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    def __init__(self, refresh_rate=250):\n",
    "        self.refresh_rate = refresh_rate\n",
    "        self.fig, self.axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        self.x_fine = np.linspace(a, b, 200)[:, None]\n",
    "        self.epoch = 0\n",
    "        self.__call__(-1)\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        if (epoch + 1) % self.refresh_rate == 0:\n",
    "            self.draw()\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time.sleep(1e-16)\n",
    "\n",
    "    def draw(self):\n",
    "        if self.epoch > 0:\n",
    "            self.axs[0].clear()\n",
    "            self.axs[0].loglog(history)\n",
    "            self.axs[0].set_title(\"epoch %d\" % (self.epoch + 1))\n",
    "\n",
    "        self.axs[1].clear()\n",
    "        self.axs[1].plot(self.x_fine, f(self.x_fine))\n",
    "        self.axs[1].plot(self.x_fine, ANN(self.x_fine, params))\n",
    "        self.axs[1].plot(xx, yy, \"o\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rcEZto-x17A3"
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Implement the GD method:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\boldsymbol{\\theta}^{(0)} \\text{given} \\\\\n",
    "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
    "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
    "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} - \\lambda \\mathbf{g}^{(k)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where N is the number of training samples. At each iteration, append the current cost to the list `history`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Use `jax.jit` to speedup the evaluation of the loss and of the gradients.\n",
    "- To us the visualization callback, just initialize it outside the training loop with:\n",
    "\n",
    "```python\n",
    "cb = Callback(refresh_rate = 250)\n",
    "```\n",
    "\n",
    "and after each training epoch call `cb(epoch)`, where `epoch` is the epoch index. Finally, call `cb.draw()` when training is over.\n",
    "\n",
    "Test you code with:\n",
    "\n",
    "- 100 training points\n",
    "- noise magnitude 0.05\n",
    "- two hidden layers of 5 neurons each\n",
    "- 2000 epochs\n",
    "\n",
    "Experiment different choices of learning rate $\\lambda$, trying to maximize the performance of the algorithm.\n",
    "\n",
    "Does it look like your ANN is struggling to learn? Try to figure out why and figure out how to fix this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18805,
     "status": "ok",
     "timestamp": 1670660675929,
     "user_tz": -60
    },
    "id": "-zOm3Bjub1FY",
    "outputId": "32619e26-02da-464d-bd39-b02233eeb81e"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_training_points = 100\n",
    "noise = 0.05\n",
    "# Hyperparameters\n",
    "layers_size = [1, 5, 5, 1]\n",
    "# Training options\n",
    "num_epochs = 2000\n",
    "learning_rate = 1e-1\n",
    "########################################\n",
    "\n",
    "xx, yy = get_training_data(n_training_points, noise)\n",
    "params = initialize_params(layers_size)\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))\n",
    "\n",
    "history = list()\n",
    "history.append(loss_jit(xx, yy, params))\n",
    "\n",
    "cb = Callback(refresh_rate=200)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    grads = grad_jit(xx, yy, params)\n",
    "    for i in range(len(params)):\n",
    "        params[i] -= learning_rate * grads[i]\n",
    "    history.append(loss_jit(xx, yy, params))\n",
    "    cb(epoch)\n",
    "cb.draw()\n",
    "\n",
    "print(\"loss: %1.3e\" % history[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dZybCJC3GhwW"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Implement the SGD method:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\boldsymbol{\\theta}^{(0)} \\text{given} \\\\\n",
    "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
    "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{|I_k|} \\sum_{i \\in I_k} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
    "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} - \\lambda_k \\mathbf{g}^{(k)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $I_k$ is the current minibatch. To select it, use the function [np.random.choice](https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html) with replacement.\n",
    "\n",
    "Consider a linear decay of the learning rate:\n",
    "\n",
    "$$\n",
    "\\lambda_k = \\max\\left(\\lambda_{\\textnormal{min}}, \\lambda_{\\textnormal{max}} \\left(1 - \\frac{k}{K}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Test different choices of batch size and try to optimize the learning rate decay strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 77219,
     "status": "ok",
     "timestamp": 1670660753130,
     "user_tz": -60
    },
    "id": "qM5HdIggAi9k",
    "outputId": "de8d7128-136d-4dd6-863e-a5afbaad6d2a"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_training_points = 100\n",
    "noise = 0.05\n",
    "# Hyperparameters\n",
    "layers_size = [1, 5, 5, 1]\n",
    "# Training options\n",
    "num_epochs = 20000\n",
    "learning_rate_max = 1e-1\n",
    "learning_rate_min = 2e-2\n",
    "learning_rate_decay = 10000\n",
    "batch_size = 10\n",
    "########################################\n",
    "\n",
    "xx, yy = get_training_data(n_training_points, noise)\n",
    "params = initialize_params(layers_size)\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))\n",
    "\n",
    "history = list()\n",
    "history.append(loss_jit(xx, yy, params))\n",
    "\n",
    "cb = Callback(refresh_rate=250)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    learning_rate = max(\n",
    "        learning_rate_min, learning_rate_max * (1 - epoch / learning_rate_decay)\n",
    "    )\n",
    "    idxs = np.random.choice(n_training_points, batch_size)\n",
    "    grads = grad_jit(xx[idxs, :], yy[idxs, :], params)\n",
    "    for i in range(len(params)):\n",
    "        params[i] -= learning_rate * grads[i]\n",
    "\n",
    "    history.append(loss_jit(xx, yy, params))\n",
    "    cb(epoch)\n",
    "cb.draw()\n",
    "\n",
    "print(\"loss: %1.3e\" % history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvXOEd7PIxcF"
   },
   "source": [
    "## Stochastic Gradient Descent with momentum\n",
    "\n",
    "Implement the SGD method with momentum:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\boldsymbol{\\theta}^{(0)} \\text{given}, \\mathbf{v}^{(0)}=\\mathbf{0}  \\\\\n",
    "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
    "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{|I_k|} \\sum_{i \\in I_k} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
    "& \\qquad \\mathbf{v}^{(k+1)} = \\alpha \\mathbf{v}^{(k)} -  \\lambda_k \\mathbf{g}^{(k)}\\\\\n",
    "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} + \\mathbf{v}^{(k+1)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Test different choices of $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 74363,
     "status": "ok",
     "timestamp": 1670660827482,
     "user_tz": -60
    },
    "id": "OCcFDUsZHYkt",
    "outputId": "347a40b6-3b93-4e2b-ddc7-da326208f189"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_training_points = 100\n",
    "noise = 0.05\n",
    "# Hyperparameters\n",
    "layers_size = [1, 5, 5, 1]\n",
    "# Training options\n",
    "num_epochs = 20000\n",
    "learning_rate_max = 1e-1\n",
    "learning_rate_min = 1e-2\n",
    "learning_rate_decay = 10000\n",
    "batch_size = 10\n",
    "alpha = 0.9\n",
    "########################################\n",
    "\n",
    "xx, yy = get_training_data(n_training_points, noise)\n",
    "params = initialize_params(layers_size)\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))\n",
    "\n",
    "history = list()\n",
    "history.append(loss_jit(xx, yy, params))\n",
    "\n",
    "cb = Callback(refresh_rate=250)\n",
    "\n",
    "# FILL HERE: initialize velocity\n",
    "for epoch in range(num_epochs):\n",
    "    learning_rate = max(\n",
    "        learning_rate_min, learning_rate_max * (1 - epoch / learning_rate_decay)\n",
    "    )\n",
    "    idxs = np.random.choice(n_training_points, batch_size)\n",
    "    grads = grad_jit(xx[idxs, :], yy[idxs, :], params)\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        # FILL HERE: compute velocity[i]\n",
    "        # FILL HERE: update params[i]\n",
    "\n",
    "    history.append(loss_jit(xx, yy, params))\n",
    "    cb(epoch)\n",
    "cb.draw()\n",
    "\n",
    "print(\"loss: %1.3e\" % history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naBVdi897MeQ"
   },
   "source": [
    "## AdaGrad\n",
    "\n",
    "Implement the AdaGrad algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\boldsymbol{\\theta}^{(0)} \\text{given}, \\mathbf{r}^{(0)}=\\mathbf{0}  \\\\\n",
    "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
    "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{|I_k|} \\sum_{i \\in I_k} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
    "& \\qquad \\mathbf{r}^{(k+1)} = \\mathbf{r}^{(k)} + \\mathbf{g}^{(k)} \\odot \\mathbf{g}^{(k)}\\\\\n",
    "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} -\n",
    "\\frac{\\lambda}{\\delta + \\sqrt{\\mathbf{r}^{(k+1)}}} \\odot \\mathbf{g}^{(k)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\delta = 10^{-7}$ is a small constant.\n",
    "Notice that the operations in the last line should be intepreted \"componentwise\".\n",
    "test different choices of the learning rate $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 75919,
     "status": "ok",
     "timestamp": 1670660903380,
     "user_tz": -60
    },
    "id": "i1rZ3K2Y1gvy",
    "outputId": "4dd9245e-4dca-479c-b708-432bc151d61f"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_training_points = 100\n",
    "noise = 0.05\n",
    "# Hyperparameters\n",
    "layers_size = [1, 5, 5, 1]\n",
    "# Training options\n",
    "num_epochs = 20000\n",
    "batch_size = 10\n",
    "learning_rate = 1e-1\n",
    "delta = 1e-7\n",
    "########################################\n",
    "\n",
    "xx, yy = get_training_data(n_training_points, noise)\n",
    "params = initialize_params(layers_size)\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))\n",
    "\n",
    "history = list()\n",
    "history.append(loss_jit(xx, yy, params))\n",
    "\n",
    "cb = Callback(refresh_rate=250)\n",
    "\n",
    "# FILL HERE: initialize cumulated_square_grad\n",
    "for epoch in range(num_epochs):\n",
    "    idxs = np.random.choice(n_training_points, batch_size)\n",
    "    grads = grad_jit(xx[idxs, :], yy[idxs, :], params)\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        # FILL HERE: update cumulated_square_grad[i]\n",
    "        # FILL HERE: update params[i]\n",
    "\n",
    "    history.append(loss_jit(xx, yy, params))\n",
    "    cb(epoch)\n",
    "cb.draw()\n",
    "\n",
    "print(\"loss: %1.3e\" % history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMDQBMrF8LU8"
   },
   "source": [
    "## RMSProp\n",
    "\n",
    "Implement the RMSProp algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\boldsymbol{\\theta}^{(0)} \\text{given}, \\mathbf{r}^{(0)}=\\mathbf{0}  \\\\\n",
    "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
    "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{|I_k|} \\sum_{i \\in I_k} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
    "& \\qquad \\mathbf{r}^{(k+1)} = \\rho \\mathbf{r}^{(k)} + (1 - \\rho)\\mathbf{g}^{(k)} \\odot \\mathbf{g}^{(k)}\\\\\n",
    "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} -\n",
    "\\frac{\\lambda}{\\delta + \\sqrt{\\mathbf{r}^{(k+1)}}} \\odot \\mathbf{g}^{(k)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\delta = 10^{-7}$ is a small constant.\n",
    "test different choices of the learning rate $\\lambda$ and decay rate $\\rho$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 80183,
     "status": "ok",
     "timestamp": 1670660983537,
     "user_tz": -60
    },
    "id": "_DrOI5bj3r76",
    "outputId": "6dd3c9f5-fd37-41c2-fc60-cbbc7c638958"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_training_points = 100\n",
    "noise = 0.05\n",
    "# Hyperparameters\n",
    "layers_size = [1, 5, 5, 1]\n",
    "# Training options\n",
    "num_epochs = 20000\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "decay_rate = 0.9\n",
    "delta = 1e-7\n",
    "########################################\n",
    "\n",
    "xx, yy = get_training_data(n_training_points, noise)\n",
    "params = initialize_params(layers_size)\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))\n",
    "\n",
    "history = list()\n",
    "history.append(loss_jit(xx, yy, params))\n",
    "\n",
    "cb = Callback(refresh_rate=250)\n",
    "\n",
    "# FILL HERE: initialize cumulated_square_grad\n",
    "for epoch in range(num_epochs):\n",
    "    idxs = np.random.choice(n_training_points, batch_size)\n",
    "    grads = grad_jit(xx[idxs, :], yy[idxs, :], params)\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        # FILL HERE: update cumulated_square_grad[i] (with decay)\n",
    "        # FILL HERE: update params[i]\n",
    "\n",
    "    history.append(loss_jit(xx, yy, params))\n",
    "    cb(epoch)\n",
    "cb.draw()\n",
    "\n",
    "print(\"loss: %1.3e\" % history[-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNWyrhuZQjv6mrFmLCtks9",
   "mount_file_id": "1jPUgULFGwoy5C8YoJgAzpAJgjvsLC5RD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
