{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZCDNUgw0nI3"
   },
   "source": [
    "# Newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1670934505648,
     "user_tz": -60
    },
    "id": "IJcJCwFhc8cs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import time\n",
    "\n",
    "# We enable double precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qkdJdGg1AP1"
   },
   "source": [
    "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
    "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670934505649,
     "user_tz": -60
    },
    "id": "c0h8ihCddDPf"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "np.random.seed(0)\n",
    "A = np.random.randn(n, n)\n",
    "x_ex = np.random.randn(n)\n",
    "b = A @ x_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UanVhF4xAVoX"
   },
   "source": [
    "Define the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1670934506647,
     "user_tz": -60
    },
    "id": "5bIQ9Y3CdbwW",
    "outputId": "3d16e09e-dacf-4d65-8f07-28d00b69c847"
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.square(A @ x - b))\n",
    "\n",
    "\n",
    "print(loss(x_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZ9XGaiAs3X"
   },
   "source": [
    "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670934506647,
     "user_tz": -60
    },
    "id": "KflmuLXld2T4"
   },
   "outputs": [],
   "source": [
    "grad = jax.grad(loss)\n",
    "hess = jax.jacfwd(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMg8ocDBndO"
   },
   "source": [
    "Check that the results are correct (up to machine precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1264,
     "status": "ok",
     "timestamp": 1670934510887,
     "user_tz": -60
    },
    "id": "xZulGRQ1efFP",
    "outputId": "2876af07-7320-4946-c2d2-31b75d849733"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x_guess = np.random.randn(n)\n",
    "\n",
    "G_ad = grad_jit(x_guess)\n",
    "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
    "print(np.linalg.norm(G_ad - G_ex))\n",
    "\n",
    "H_ad = hess_jit(x_guess)\n",
    "H_ex = 2 * A.T @ A\n",
    "print(np.linalg.norm(H_ad - H_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-gA_kKPB2SV"
   },
   "source": [
    "Exploit the formula\n",
    "\n",
    "$$\n",
    "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "$$\n",
    "\n",
    "to write an optimized function returning the hessian-vector-product\n",
    "\n",
    "$$\n",
    "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n",
    "$$\n",
    "\n",
    "Compare the computational performance w.r.t. the full hessian computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1670934517725,
     "user_tz": -60
    },
    "id": "T9969dU4kc6f",
    "outputId": "1f6d6a58-d1ea-4b0c-cfe4-6609d995d0d2"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "v = np.random.randn(n)\n",
    "\n",
    "hvp_basic = lambda x, v: hess(x) @ v\n",
    "gvp = lambda x, v: jnp.dot(grad(x), v)\n",
    "hvp = lambda x, v: jax.grad(gvp, argnums=0)(x, v)\n",
    "\n",
    "hvp_basic_jit = jax.jit(hvp_basic)\n",
    "hvp_jit = jax.jit(hvp)\n",
    "\n",
    "Hv_ad = hvp_jit(x_guess, v)\n",
    "Hv_ex = H_ex @ v\n",
    "print(np.linalg.norm(Hv_ad - Hv_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17317,
     "status": "ok",
     "timestamp": 1670743186289,
     "user_tz": -60
    },
    "id": "jsA4eUnuj3ju",
    "outputId": "cbd398c7-dfc3-4ced-ade7-4cbbb7a280a7"
   },
   "outputs": [],
   "source": [
    "%timeit hvp_basic_jit(x_guess, v)\n",
    "%timeit hvp_jit(x_guess, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TagmrdjG4Ww4"
   },
   "source": [
    "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670743186290,
     "user_tz": -60
    },
    "id": "jZQBCmXBhBwu",
    "outputId": "76d4bb2a-f683-422c-adee-978deaba1511"
   },
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "elapsed_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    # FILL HERE: compute the gradient `G`, the hessian `H` and the increment `incr`\n",
    "    elapsed_time += time.time() - t0\n",
    "    x += incr\n",
    "\n",
    "    print(\"============ epoch %d\" % epoch)\n",
    "    print(\"loss: %1.3e\" % loss_jit(x))\n",
    "    print(\"grad: %1.3e\" % np.linalg.norm(G))\n",
    "    print(\"incr: %1.3e\" % np.linalg.norm(incr))\n",
    "\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        break\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} [s]\")\n",
    "rel_err = np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)\n",
    "print(f\"Relative error: {rel_err:1.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(matvec_fn, b, tol=1e-12, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient Solver for Ax = b, where A is only accessible via matvec_fn.\n",
    "    \"\"\"\n",
    "    x = jnp.zeros_like(b)\n",
    "    r = b - matvec_fn(x)  # Residual\n",
    "    p = r\n",
    "    rsold = jnp.dot(r, r)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        Ap = matvec_fn(p)\n",
    "        alpha = rsold / jnp.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rsnew = jnp.dot(r, r)\n",
    "        if jnp.sqrt(rsnew) < tol:\n",
    "            break\n",
    "        p = r + (rsnew / rsold) * p\n",
    "        rsold = rsnew\n",
    "    return x\n",
    "\n",
    "\n",
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "elapsed_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    # FILL HERE: compute the gradient `G` and the increment `incr` by using the CG\n",
    "    #            you should never compute explicitly the hessian\n",
    "    elapsed_time += time.time() - t0\n",
    "    x += incr\n",
    "\n",
    "    print(\"============ epoch %d\" % epoch)\n",
    "    print(\"loss: %1.3e\" % loss_jit(x))\n",
    "    print(\"grad: %1.3e\" % np.linalg.norm(G))\n",
    "    print(\"incr: %1.3e\" % np.linalg.norm(incr))\n",
    "\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        break\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} [s]\")\n",
    "rel_err = np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)\n",
    "print(f\"Relative error: {rel_err:1.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNL7303C4oTL"
   },
   "source": [
    "Repeat the optimization loop for the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOZZWrn3iEgI"
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum((A @ x - b) ** 4)\n",
    "\n",
    "\n",
    "grad = jax.grad(loss)\n",
    "hess = jax.jacfwd(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1670743187191,
     "user_tz": -60
    },
    "id": "-hZyFgjQiOvB",
    "outputId": "3d280332-6603-450f-a977-0fc768fd8be8"
   },
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "hist = [loss_jit(x)]\n",
    "t0 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    l = loss_jit(x)\n",
    "    hist.append(l)\n",
    "    H = hess_jit(x)\n",
    "    G = grad_jit(x)\n",
    "    incr = np.linalg.solve(H, -G)\n",
    "    x += incr\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        print(\"convergence reached!\")\n",
    "        break\n",
    "ttot = time.time() - t0\n",
    "\n",
    "plt.semilogy(hist, \"o-\")\n",
    "print(\"epochs: %d\" % epoch)\n",
    "print(\"relative error: %1.3e\" % (np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-Newton Optimization with BFGS Update\n",
    "\n",
    "This algorithm minimizes an objective function $ f(\\mathbf{x}) $ using a quasi-Newton method with the BFGS update. The goal is to iteratively update the solution $ \\mathbf{x} $ and approximate the inverse Hessian until convergence criteria are met.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. **Initialization**:\n",
    "\n",
    "   - Set the initial guess $ \\mathbf{x} = \\mathbf{x}\\_{\\text{guess}} $.\n",
    "   - Let $ \\mathbf{I} $ be the identity matrix, and initialize $ B^{-1} = \\mathbf{I} $.\n",
    "   - Compute the initial gradient $ \\nabla f = \\nabla f(\\mathbf{x}\\_{\\text{guess}}) $.\n",
    "   - Initialize the loss history: $ \\text{history} = [f(\\mathbf{x}_{\\text{guess}})] $.\n",
    "   - Set $ \\text{epoch} = 0 $.\n",
    "\n",
    "   $$\n",
    "   B^{-1} = \\mathbf{I}, \\quad \\nabla f = \\nabla f(\\mathbf{x}_{\\text{guess}}), \\quad \\text{history} = [f(\\mathbf{x}_{\\text{guess}})]\n",
    "   $$\n",
    "\n",
    "2. **Iterative Updates**:\n",
    "\n",
    "   - While $ \\|\\nabla f\\| > \\text{tol} $ and $ \\text{epoch} < \\text{max\\_epochs} $:\n",
    "\n",
    "     - Increment the epoch counter:\n",
    "\n",
    "       $$\n",
    "       \\text{epoch} \\leftarrow \\text{epoch} + 1\n",
    "       $$\n",
    "\n",
    "     - Compute the search direction:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{p} = -B^{-1} \\nabla f\n",
    "       $$\n",
    "\n",
    "     - Perform a line search to find the step size $ \\alpha $ using `sp.optimize.line_search`:\n",
    "\n",
    "       $$\n",
    "       \\alpha \\leftarrow \\text{line\\_search}(f, \\nabla f, \\mathbf{x}, \\mathbf{p})\n",
    "       $$\n",
    "\n",
    "       If $ \\alpha $ is not found, set $ \\alpha = 10^{-8} $.\n",
    "\n",
    "     - Update the solution vector:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}\n",
    "       $$\n",
    "\n",
    "     - Compute the displacement:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{s} = \\mathbf{x}_{\\text{new}} - \\mathbf{x}\n",
    "       $$\n",
    "\n",
    "     - Update $ \\mathbf{x} $:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{x} \\leftarrow \\mathbf{x}_{\\text{new}}\n",
    "       $$\n",
    "\n",
    "     - Compute the new gradient and gradient difference:\n",
    "\n",
    "       $$\n",
    "       \\nabla f_{\\text{new}} = \\nabla f(\\mathbf{x}), \\quad \\mathbf{y} = \\nabla f_{\\text{new}} - \\nabla f\n",
    "       $$\n",
    "\n",
    "       Update $ \\nabla f $:\n",
    "\n",
    "       $$\n",
    "       \\nabla f \\leftarrow \\nabla f_{\\text{new}}\n",
    "       $$\n",
    "\n",
    "     - Compute the scalar $ \\rho $:\n",
    "\n",
    "       $$\n",
    "       \\rho = \\frac{1}{\\mathbf{y}^\\top \\mathbf{s}}\n",
    "       $$\n",
    "\n",
    "     - Update the inverse Hessian approximation $ B^{-1} $ using the Sherman–Morrison formula:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{E} = \\mathbf{I} - \\rho \\mathbf{y} \\mathbf{s}^\\top\n",
    "       $$\n",
    "\n",
    "       $$\n",
    "       B^{-1} \\leftarrow \\mathbf{E}^\\top B^{-1} \\mathbf{E} + \\rho \\mathbf{s} \\mathbf{s}^\\top\n",
    "       $$\n",
    "\n",
    "     - Append the current loss value to history:\n",
    "       $$\n",
    "       \\text{history.append}(f(\\mathbf{x}))\n",
    "       $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "tol = 1e-8\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "# FILL HERE: initialize\n",
    "# - x\n",
    "# - Binv\n",
    "# - grad\n",
    "# - history \n",
    "\n",
    "# FILL HERE: the BFGS algorithm\n",
    "# while ...\n",
    "    # 1. search direction\n",
    "    # 2. line search and compute x_new\n",
    "    # 3. computing s and y\n",
    "    # 4. Sherman–Morrison update\n",
    "\n",
    "plt.semilogy(history, \"o-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsWkGVhufqPYWuaAVLQS75",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
