{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine with OSQP\n",
    "\n",
    "In this exercise, we will implement a Support Vector Machine (SVM) using the OSQP (Operator Splitting Quadratic Program) solver. We will focus on the Radial Basis Function (RBF) kernel and visualize the decision boundary and support vectors.\n",
    "\n",
    "## Objectives\n",
    "- Implement SVM with an RBF kernel using OSQP.\n",
    "- Solve the SVM optimization problem.\n",
    "- Compute the decision boundary and support vectors.\n",
    "- Visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We will begin by importing the necessary libraries. JAX will be used for numerical computations, OSQP for optimization, and Matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import BoxOSQP\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the RBF Kernel\n",
    "\n",
    "The Radial Basis Function (RBF) kernel is defined as follows:\n",
    "$$ K(x, y) = \\exp(-\\gamma \\|x - y\\|^2) $$\n",
    "\n",
    "Where $ \\gamma $ is a parameter that determines the spread of the kernel. We will implement this kernel as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, gamma):\n",
    "    # This must work when X1 and X2 are matrices\n",
    "    # each element of the kernel, given two vectors x1, x2 as inputs is\n",
    "    # exp(- \\gamma (||x_1||^2 + ||x_2||^2 - 2 <x_1, x_2>))\n",
    "    # Compute ||x_1||^2, ||x_2||^2, <x_1, x_2> separatly with X1, X2 matrices\n",
    "    # and than combine them together (use broadcasting)\n",
    "\n",
    "    # if X1 has shape (n1, dim) and X2 has shape (n2, dim)\n",
    "    # the output shall have shape (n1, n2)\n",
    "\n",
    "    # FILL HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the SVM Optimization with OSQP\n",
    "\n",
    "The objective of the Support Vector Machine (SVM) is to find a hyperplane that separates data points of different classes with the maximum margin. The formulation of the SVM optimization problem can be derived as follows:\n",
    "\n",
    "### Dual Problem\n",
    "\n",
    "The dual form of the SVM optimization problem is derived from the primal problem using dual coefficients (see Representer Theorem). The dual problem is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text{minimize} && \\frac{1}{2} \\beta^T K \\beta - \\beta^T \\mathbf{y}  \\\\\n",
    "& \\text{subject to} && \\sum_{i=1}^{n} \\beta_i = 0 \\\\\n",
    "& && -C \\leq \\beta_i \\leq 0, \\quad \\forall i \\text{ if } y_i =-1\\\\\n",
    "& && 0 \\leq \\beta_i \\leq C, \\quad \\forall i \\text{ if } y_i =1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\beta_i $ are the dual coefficients.\n",
    "- $ K(X_i, X_j) $ is the kernel function, which in our case is the Radial Basis Function (RBF) kernel defined as:\n",
    "  \n",
    "$$\n",
    "K(X_i, X_j) = \\exp(-\\gamma \\|X_i - X_j\\|^2)\n",
    "$$\n",
    "\n",
    "### OSQP Formulation\n",
    "\n",
    "In the context of OSQP, we reformulate the dual problem in terms of matrix-vector products for efficient computation. The dual optimization problem is rewritten as:\n",
    "\n",
    "1. **Quadratic Objective**:\n",
    "\n",
    "   $$\n",
    "   \\text{minimize} \\quad \\frac{1}{2} \\mathbf{\\beta}^T K \\mathbf{\\beta} - \\mathbf{y}^T \\mathbf{\\beta}\n",
    "   $$\n",
    "\n",
    "\n",
    "2. **Linear Constraints**:\n",
    "\n",
    "   $$\n",
    "   A \\mathbf{\\beta} = \\mathbf{z}\n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   l \\leq \\mathbf{z} \\leq u\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ A $ is the constraint matrix.\n",
    "   - $ l $ and $ u $ are the lower and upper bounds for $ \\beta_i $, derived from the constraints of the dual problem.\n",
    "\n",
    "By using the OSQP solver, we can efficiently solve this quadratic program to obtain the optimal values of $ \\beta $, which can then be used to compute the weight vector $ \\beta $ and the bias $ b $ for classification.\n",
    "\n",
    "In our case we have that \n",
    "- $A$: is the identity\n",
    "- We implement the equality constraint on the sum of $\\beta$ as $0 \\leq \\sum_{i=0}^n \\beta_i \\leq 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_kernel_svm_osqp(X, y, C, gamma):\n",
    "    K = rbf_kernel(X, X, gamma)\n",
    "\n",
    "    # 0.5 beta^T K beta \n",
    "    def matvec_Q(Q, beta):\n",
    "        return jnp.dot(Q, beta)\n",
    "\n",
    "    # return beta and the sum of betas, for the constrait\n",
    "    def matvec_A(_, beta):\n",
    "        return beta, jnp.sum(beta)\n",
    "\n",
    "    # first element of the tuple is the elementwise constraint on \\beta_i\n",
    "    # second element of the tuple is the constraint on the sum of betas\n",
    "    # the tuple has size two like the size of the tuple returned by `matvec_A`\n",
    "    l = -jax.nn.relu(-y * C), 0.\n",
    "    u = jax.nn.relu(y * C), 0.\n",
    "\n",
    "    # build and run the optimizer\n",
    "    osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=1e-6)\n",
    "    params, _ = osqp.run(\n",
    "        init_params=None,\n",
    "        params_obj=(K, -y), # parameters passed to `matvec_Q` and used for `y^T \\beta`\n",
    "        params_eq=None,\n",
    "        params_ineq=(l, u)\n",
    "    )\n",
    "    beta = params.primal[0]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute the Bias Term\n",
    "\n",
    "The bias term $b$ is computed using one of the support vectors. The formula used is:\n",
    "\n",
    "$$ b = y_i - \\sum_{j} \\beta_j K(x_j, x_i) $$\n",
    "\n",
    "Where $i$ is the index of a support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias(X, y, beta, gamma):\n",
    "    # Compute the bias term b by using a support vector\n",
    "    support_indices = jnp.where(jnp.abs(beta) > 1e-4)[0]\n",
    "    if len(support_indices) > 0:\n",
    "        i = support_indices[0]\n",
    "        # https://stats.stackexchange.com/questions/451868/calculating-the-value-of-b-in-an-svm\n",
    "        # the loop on SV is equivalent to a loop on all betas since beta is almost 0\n",
    "        return y[i]-jnp.sum(beta * rbf_kernel(X, X[i:i+1], gamma).reshape((-1,)))\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Decision Function\n",
    "\n",
    "The decision function computes the predicted output for a test point as follows:\n",
    "$$ \\hat{y}_i = \\sum_{i} \\beta_i y_i K(x_i, x) + b $$\n",
    "\n",
    "Where $ x_i $ are the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X_train, y_train, X_test, beta, gamma, b):\n",
    "    # FILL HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare the Dataset\n",
    "\n",
    "We will generate a circular dataset using the `make_circles` function from Scikit-learn, scale the data, and transform the labels to \\{-1, 1\\}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 30\n",
    "lam = 0.001\n",
    "gamma = 0.5  # Gamma parameter for RBF kernel\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Prepare circular dataset\n",
    "X, y = datasets.make_circles(n_samples=num_samples, factor=0.5, noise=0.05)  # Circle data\n",
    "X = preprocessing.StandardScaler().fit_transform(X)  # Scale data\n",
    "y = jnp.array(y * 2. - 1.)  # Transform labels from {0, 1} to {-1., 1.}\n",
    "\n",
    "C = 1. / 2. / lam / num_samples\n",
    "\n",
    "# Plot the data points\n",
    "# FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the SVM Model\n",
    "\n",
    "Now we will solve the SVM optimization problem using OSQP and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with OSQP\n",
    "beta = binary_kernel_svm_osqp(X, y, C, gamma)\n",
    "\n",
    "# Create a grid of points to evaluate the decision function\n",
    "h = .1  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Compute the decision function for each point in the grid\n",
    "# 1. use `jnp.c_`, which concatenate slices, scalars and array-like objects along the last axis\n",
    "#    to build X_test from `xx` and `yy`\n",
    "# 2. use `decision_function` to compute the predictions `Z`\n",
    "# FILL HERE\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k')\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')  # Decision boundary\n",
    "\n",
    "support_vectors = jnp.where(jnp.abs(beta) > 1e-4)[0]\n",
    "# Highlight the support vectors\n",
    "plt.scatter(X[support_vectors, 0], X[support_vectors, 1], s=100, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.title(\"SVM Decision Boundary and Support Vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "K = rbf_kernel(X, X, gamma)  # Compute RBF kernel matrix\n",
    "svc = svm.SVC(kernel=\"precomputed\", C=C, tol=1e-6).fit(K, y)\n",
    "beta_sk = np.zeros(K.shape[0])\n",
    "beta_sk[svc.support_] = svc.dual_coef_[0]\n",
    "Z_sk = svc.decision_function(rbf_kernel(X_test, X, gamma=gamma))\n",
    "\n",
    "print(f\"Beta discrepancy       {np.max(np.abs(beta_sk - beta)):.3e}\")\n",
    "print(f\"Intercept discrepancy  {np.max(np.abs(svc.intercept_ - b)):.3e}\")\n",
    "print(f\"Prediction discrepancy {np.max(np.abs(Z_sk - Z.flatten())):.3e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
